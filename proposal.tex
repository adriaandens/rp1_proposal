\documentclass{scrartcl}
\usepackage{multicol}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage[parfill]{parskip}
\usepackage{titlesec}

\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

\title{Improving drive-by download detection: boot. visit$^n$. kill. repeat.}
\subtitle{An RP1 proposal,\\Master in System and Network Engineering}
\author{
  Adriaan Dens\\
   \texttt{adriaan.dens@os3.nl}
  \and
  Martijn Bogaard\\
   \texttt{martijn.bogaard@os3.nl}
}

\begin{document}
\maketitle

\section{Introduction}

In the digital world of today, malware is still a massive and growing problem. While back in the day it was used to annoy users and system administrators, nowadays it's used for extortion, cyber espionage and surveillance by criminal groups and rivalling governments. One of the main risk factors to get infected with malware is a drive-by download while visiting a normal day-to-day website because, for example, the website got hacked and infected. 

% TODO Sources
In many cases\footnote{http://www.proofpoint.com/threatinsight/posts/malware-in-ad-networks-infects-visitors-and-jeopardizes-brands.php} \footnote{http://blog.fox-it.com/2013/08/01/analysis-of-malicious-advertisements-on-telegraaf-nl} \footnote{http://blog.fox-it.com/2014/01/03/malicious-advertisements-served-via-yahoo}, it was not the actual website but one of the advertisement networks that was infiltrated and which subsequently started serving malicious code hidden in innocently looking advertisement code. This is also called malvertising \cite{Li2012}.

%In many of the cases where one of the bigger national websites was involved, it was not the actual website but one of the advertisement networks that are used by such websites that was hacked or infiltrated by hiding in innocent looking advertisement malicious code. Such advertisements can then be served on many different websites.

National CERT organisations are (of course) interested in an early detection of such threats. While automated systems to scan websites already exist, like Cuckoo\footnote{http://cuckoosandbox.org} and Anubis\footnote{http://anubis.iseclab.org}, one of the main difficulties is the time needed to analyse a single website and the maintenance needed to keep these systems up-to-date for the latest threats.

\section{Research Question}
In cooperation with of the Dutch National Cyber Security Center (NCSC-NL), our research project will focus on the question:\\ %how it would be possible to dramatically improve the efficiency of scanning websites while maintaining the ability to pinpoint a single website as the source of an infection.

\textit{How can we concurrently visit multiple URLs and still be able to determine which URL was responsible for malicious activities?}\\

To answer the research question, multiple subquestions have been formulated:
\begin{itemize}
\item Which techniques are used by browsers to make concurrently visiting multiple URLs possible?
\item Which APIs are used by web browsers to make HTTP requests and retrieve webpages?
\item How can we link a HTTP request to its source URL without the modification of the used web browser?
\end{itemize}

\section{Related work}
The growing importance and economic losses of malware resulted in many research projects in the last years. The detection and analysis of malware has been researched from several different angles and resulted in many proposed static and dynamic analysis techniques.

In 2013, Le et al. \cite{Le2013} presented a framework that describes the common stages and characteristics of a drive-by download attack. They described four stages from placing the malicious content on a webpage until the execution of the malicious activity.

In 2011, a paper from Canali et al. \cite{Canali2011} was released about the problematic performance of dynamic analysis and with a solution proposed in the form of ``Prophiler''. Prophiler is a filter that deploys static analysis techniques and that is able to reduce the load with more than 85\% compared to dynamic analysis. This with a comparable amount of false negatives.

In the same year Rajab et al. \cite{Rajab11trendsin} gave an overview of the trends regarding web malware detection and how the malware tries to circumvent the detection. This research focused on the advantages and disadvantages of four techniques: Virtual Machine honeypots, Browser Emulation honeypots, Classification based on Domain Reputation and Anti-Virus Engines.

\section{Scope}

The scope of this project consists of creating an algorithm that allows multiple URLs to be opened at the same time while still being able to track all further interaction, such as unexpected HTTP requests and other malicious activity, and link them to the original request/URL. To proof that the algorithm is something feasible, we will also implement a proof of concept of the algorithm.

The goal during this research project is to make the algorithm platform agnostic. If this is not possible then we will limit our self to Windows 7 with version 8 of the Internet Explorer browser.
%The goal during this research project is to make the algorithm platform agnostic. When it will not be possible to make the algorithm platform agnostic and also during the development of the PoC we will limit our self to Windows 7 with version 8 of the Internet Explorer browser. This is at the time of writing one of the most used combinations in companies in the Netherlands.

The detection and identification of malicious behaviour is not part of this project. For our PoC we will stick to the detection of a well-known older and still to be determined malware family which existance is easy to detect on the system. 

%Inside the scope of this project is figuring out and implementing an algorithm to concurrently visit multiple URLs in one virtual machine. Secondly, we will implement a man-in-the-middle to decrypt HTTPS traffic. Multiple browser support is not in the scope of this project, we will only implement our changes in the default Windows browser, Internet Explorer, as there is already some preliminary support for it in Cuckoo.

% focus op het algoritme, probeer een kleine schets te geven van hoe jullie bijv naar kunnen kijken

\section{Approach}

To create a generic algorithm, we need to have more information than the data we see when passively sniffing the network. Only looking at the network traffic will most likely be not enough to trace malicious activity to a URL the client visited. Because we have access to the client's machine, we can retrieve extra information from the running system which allows us to give more context to the network traffic. What information is relevant has to be investigated. Firstly, we will look at how browsers work. More specifically, we look at the implementation of the tabs and the network stack of the most used browsers. How does a browser seperate the content and calls from different tabs? Does the browser implement its own network stack or does it use the operating system's network stack? By answering these questions, we can better define a generic algorithm.

Secondly, we investigate the possibilities of hooking into browsers and the operating system to be able to retrieve and get this extra information and couple it to the data send and received on the network.

Finally, we can define a generic algorithm that allows us to trace activity to a certain URL. Once we have this generic algorithm, we will implement this in a proof of concept. As to not start from scratch, we will use Cuckoo which is dynamic malware analysis system that uses a virtual environment to run malware. Conveniently, Cuckoo already supports one URL as input and also provides dozens of hooks into the operating system. 
%- algo maken die generiek is


%- Implementing

% Approach
%To implement our research in a proof of concept, we will be changing the Cuckoo source code to be able to track multiple URLs in one virtual machine. First, multiple URL support should be added to allow multiple URLs in the virtual environment. Secondly, we will have to implement the algorithm to track the changes that are the result of browsing to an URL. Finally, we will implement a man-in-the-middle to be able to view the contents of encrypted SSL/TLS tunnels.\\
% monitoren van api calls ipv netwerklayer stuff, cuckoo is hier handig voor daarom gebruiken we wss cuckoo voor PoC
% Cuckoo heeft ook al een hook systeem voor system calls

%\section{Requirements}

%As we will use and change Cuckoo, we require the Cuckoo source code as well as all dependencies needed by Cuckoo. Furthermore, a virtualized environment such as Virtualbox is needed with a Microsoft Windows virtual machine. Internet Explorer 8 will be considered as the minimum version of the browser that we use.

\section{Planning}

A meeting with the supervisors is planned every Wednesday afternoon. Therefore in week 1, we set the deadline of the proposal on Wednesday so it can be reviewed by our supervisors and be changed accordingly to the feedback given. At the end of week 1 and the beginning of week 2, time will be spend on devising an algorithm to implement concurrent analysis of URLs. After that, most of our time will be spent implementing our algorithm. In week 4, the report will be written and the presentation will be prepared.

\section{Expected results}

At the end of the project we expect to have an algorithm that answers the research question and a proof of concept of this algorithm. This PoC will be implemented by adapting the Cuckoo Sandbox. By checking multiple URLs in one virtual machine instance, we want to make Cuckoo more useable for checking hundreds of URLs. Our goal is to increase the performance of Cuckoo tenfold by implementing our algorithm.

\section{Ethical issues}

Our research contains no major ethical issues as it does not include working with personally identifiable information. Malware, if any, will be run in a controlled virtual environment. After every testrun the virtual machine will be automatically destroyed.

\bibliographystyle{ieeetr}
\bibliography{proposal}

% veel meer references: 8 to be exact
\end{document}
