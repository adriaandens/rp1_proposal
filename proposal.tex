\documentclass{scrartcl}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage[parfill]{parskip}

\title{Improving drive-by download detection: boot. visit$^n$. kill. repeat.}
\subtitle{An RP1 proposal,\\Master in System and Network Engineering}
\author{
  Adriaan Dens\\
   \texttt{adriaan.dens@os3.nl}
  \and
  Martijn Bogaard\\
   \texttt{martijn.bogaard@os3.nl}
}

\begin{document}
\maketitle

\section{Introduction}

In the digital world of today, malware is still a massive and growing problem. While back in the day it was used to annoy users and system administrators, nowadays it's used for extortion, cyber espionage and surveillance by criminal groups and rivalling governments. One of the main risk factors to get infected with malware is a drive-by download while visiting a normal day-to-day website because, for example, the website got hacked and infected. 

% TODO Sources
In many cases \cite{proofpoint}, it was not the actual website but one of the advertisement networks that was infiltrated and which subsequently started serving malicious code hidden in innocently looking advertisement code. This is also called malvertising.

%In many of the cases where one of the bigger national websites was involved, it was not the actual website but one of the advertisement networks that are used by such websites that was hacked or infiltrated by hiding in innocent looking advertisement malicious code. Such advertisements can then be served on many different websites.

National CERT organisations are (of course) interested in an early detection of such threats. While automated systems to scan websites already exist, like Cuckoo \cite{cuckoo} and Anubis \cite{http://anubis.iseclab.org/}, one of the main difficulties is the time needed to analyse a single website and the maintenance needed to keep these systems up-to-date for the latest threats.

\section{Research Question}
In cooperation with of the Dutch National Cyber Security Center (NCSC-NL), our research project will focus on the question:\\ %how it would be possible to dramatically improve the efficiency of scanning websites while maintaining the ability to pinpoint a single website as the source of an infection.

\textit{How can we concurrently visit multiple URLs and still be able to determine which URL was responsible for malicious activities?}\\

To answer the research question multiple subquestions have been formulated:
 
\begin{itemize}

\item Which techniques are used by browsers to make concurrently visiting multiple URLs possible?
\item Which APIs are used by webbrowsers to make HTTP requests and retreive webpages?
\item How can we link a HTTP request to its source URL without the modification of the used webbrowser?

\end{itemize}

\section{Related work}

% google scholar, dude van os3 uit 2011 paper, 
% Wat is cuckoo
% Waarom is cuckoo relevant aan ons onderzoek
% References naar virustotal en malwr
Most existing systems for malware detection are passive or only able to scan a single website at a time. Several approaches for such systems are documented in the literature \cite{auto_malware}. Cuckoo \cite{cuckoo} is one of the most popular dynamic analysis systems and is used by Virustotal \cite{virustotal} and Malwr \cite{malwr}. Cuckoo executes the malware in a virtual machine and analyses its behaviour.

% Source Honey spider
The predecessor of NCSC-NL started in 2007 with the development of their own system, namely the Honeyspider network \cite{honeyspider}, for the dynamic analysis of websites. This system crawled the top X websites of the Netherlands on a daily base. The downside of this system is that it requires a lot of maintenance and hence it started to become outdated.

% Boeit niet
In 2011, the development was started of the second generation of this platform. This time, it was decided to develop the platform as part of a joint venture between NASK/CERT Polska (Poland) and NCSC-NL. This system is build around existing solutions like Cuckoo.

\section{Scope}

The scope of this project consists of creating an algorithm that allows multiple URLs to be opened at the same time while still being able to track all further interaction, such as unexpected HTTP requests and other malicious activity, and link them to the original request/URL. To proof that the algorithm is something feasible, we will try to also implement a proof of concept of the algorithm.

The goal during this research project is to make the algorithm platform agnostic. When this is not possible and during the development of the PoC we will focus on Windows 7 with version 8 of the Internet Explorer browser. This is at the time of writing one of the most used combinations in companies in the Netherlands.

The detection and identification of malicious behaviour is not part of this project. For our PoC we will stick to the detection of a well-known older and still to be determined malware family which existance is easy to detect on the system. 

%Inside the scope of this project is figuring out and implementing an algorithm to concurrently visit multiple URLs in one virtual machine. Secondly, we will implement a man-in-the-middle to decrypt HTTPS traffic. Multiple browser support is not in the scope of this project, we will only implement our changes in the default Windows browser, Internet Explorer, as there is already some preliminary support for it in Cuckoo.

% focus op het algoritme, probeer een kleine schets te geven van hoe jullie bijv naar kunnen kijken

\section{Approach}



% Approach
%To implement our research in a proof of concept, we will be changing the Cuckoo source code to be able to track multiple URLs in one virtual machine. First, multiple URL support should be added to allow multiple URLs in the virtual environment. Secondly, we will have to implement the algorithm to track the changes that are the result of browsing to an URL. Finally, we will implement a man-in-the-middle to be able to view the contents of encrypted SSL/TLS tunnels.\\
% monitoren van api calls ipv netwerklayer stuff, cuckoo is hier handig voor daarom gebruiken we wss cuckoo voor PoC
% Cuckoo heeft ook al een hook systeem voor system calls

%\section{Requirements}

%As we will use and change Cuckoo, we require the Cuckoo source code as well as all dependencies needed by Cuckoo. Furthermore, a virtualized environment such as Virtualbox is needed with a Microsoft Windows virtual machine. Internet Explorer 8 will be considered as the minimum version of the browser that we use.

\section{Planning}

A meeting with the supervisors is planned every Wednesday afternoon. Therefore in week 1, we set the deadline of the proposal on Wednesday so it can be reviewed by our supervisors and be changed accordingly to the feedback given. At the end of week 1 and the beginning of week 2, time will be spend on devising an algorithm to implement concurrent analysis of URLs. After that, most of our time will be spent implementing our algorithm. In week 4, the report will be written and the presentation will be prepared.

\section{Expected results}

At the end of the project we expect to have an algorithm that answers the research question and a proof of concept of this algorithm. This PoC will be implemented by adapting the Cuckoo Sandbox. By checking multiple URLs in one virtual machine instance, we want to make Cuckoo more useable for checking hundreds of URLs. Our goal is to increase the performance of Cuckoo tenfold by implementing our algorithm.

\section{Ethical issues}

Our research contains no major ethical issues as it does not include working with personally identifiable information. Malware, if any, will be run in a controlled virtual environment. After every testrun the virtual machine will be automatically destroyed.

\bibliographystyle{ieeetr}
\bibliography{proposal}

% veel meer references: 8 to be exact

\end{document}
