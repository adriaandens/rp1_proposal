\documentclass{scrartcl}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage[parfill]{parskip}

\title{Improving drive-by download detection: boot. visit$^n$. kill. repeat.}
%\subtitle{}
\author{
  Adriaan Dens\\
   \texttt{adriaan.dens@os3.nl}
  \and
  Martijn Bogaard\\
   \texttt{martijn.bogaard@os3.nl}
}

\begin{document}
\maketitle
\newpage

\section{Introduction}

In the digital world of today is malware a massive and still growing problem. While in the beginning it was used to annoy users and system administrators, nowadays it's used for extortion, cyber espionage and surveillance by criminal groups and rivaling governments. One of the main risk factors to get infected with malware is a drive-by download while visiting a normal website, for example because the website was hacked and infected. 

Several major websites in the Netherlands have been hacked in the past and started to serve malware. While this was usually detected in a matter of hours, this still resulted in thousands of infections.

In many of the cases where one of the bigger national websites have been involved was not the actual website but one of the advertisement networks that are used by such websites hacked or infiltrated by hiding in an innocent looking advertisement malicious code. Such advertisements can then be served on many different websites.

National CERT organisations like the department of NCSC have a growing interest in an early detection of such treats. While automated systems to scan websites exist already for many years is one of the main difficulties the time needed to analyse a single website and the maintenance needed to keep it up-to-date for the latest treats. On request of NCSC will this research project focus on the question how it would be possible to dramaticly improve the efficiency of scanning websites while maintaining the ability to pinpoint a single website as source of the infection.

\section{Research Question}

\textit{Is it possible to concurrently visit multiple URLs in one virtual machine and still be able to determine which URL was responsible for a malicious advertisement?}
\\
\section{Related work}



Honeyspider 1 en 2

\section{Scope}

	Cuckoo aangepast hebben met de volgende features
		- Multiple URL support
		- HTTPS decryption
		- meerdere url's in één VM steken en toch alles kunnen tracken van waar iets kwam


 - Enkel met IE laten werken of supporten we ook andere browsers?
	(python submit.py --url example.org --browser chrome ?????)
 - Enkel in Virtualbox laten werken of overal


\section{Approach and methods}

uitleggen van hoe we die concurrency gaan doen

gebruik maken van wat al bestaat: Cuckoo
open source op github


\section{Requirements}

we gebruiken Cuckoo
	- dependencies van cuckoo
	- woordje over Python ofzo?
	- virtualized environments
Windows machines zijn dus ook nodig


\section{Planning}

elke woensdag (wss) naar den haag in de namiddag voor meeting
eerste week tegen woensdag proposal af en hoe we die concurrency gaan oplossen
 

\section{Expected results}
	Run top 100 NL websites elk halfuur?
	Cuckoo aangepast hebben met de volgende features
		- Multiple URL support
		- HTTPS decryption
		- meerdere url's in één VM steken en toch alles kunnen tracken van waar iets kwam

\section{Ethical issues}
	we werken niet met persoonlijke data dus geen ethical issues...

\bibliographystyle{ieeetr}
\bibliography{proposal}

\end{document}
