\documentclass{scrartcl}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage[parfill]{parskip}

\title{Improving drive-by download detection: boot. visit$^n$. kill. repeat.}
\subtitle{An RP1 proposal,\\Master in System and Network Engineering}
\author{
  Adriaan Dens\\
   \texttt{adriaan.dens@os3.nl}
  \and
  Martijn Bogaard\\
   \texttt{martijn.bogaard@os3.nl}
}

\begin{document}
\maketitle

\section{Introduction}

In the digital world of today is malware a massive and still growing problem. While in the beginning it was used to annoy users and system administrators, nowadays it's used for extortion, cyber espionage and surveillance by criminal groups and rivaling governments. One of the main risk factors to get infected with malware is a drive-by download while visiting a normal website, for example because the website was hacked and infected. 

Several major websites in the Netherlands have been hacked in the past and started to serve malware. While this was usually detected in a matter of hours, this still resulted in thousands of infections.

In many of the cases where one of the bigger national websites have been involved was not the actual website but one of the advertisement networks that are used by such websites hacked or infiltrated by hiding in an innocent looking advertisement malicious code. Such advertisements can then be served on many different websites.

National CERT organisations like the department of NCSC have a growing interest in an early detection of such treats. While automated systems to scan websites exist already for many years is one of the main difficulties the time needed to analyse a single website and the maintenance needed to keep it up-to-date for the latest treats. On request of NCSC will this research project focus on the question how it would be possible to dramaticly improve the efficiency of scanning websites while maintaining the ability to pinpoint a single website as source of the infection.

\section{Research Question}

\textit{How can we do massive malware scanning in a timely fashion?}



%\textit{How can we concurrently visit multiple URLs in one virtual machine and still be able to determine which URL was responsible for a malicious advertisement?}




\section{Related work}



Honeyspider 1 en 2

\section{Scope}

	Cuckoo aangepast hebben met de volgende features
		- Multiple URL support
		- HTTPS decryption
		- meerdere url's in één VM steken en toch alles kunnen tracken van waar iets kwam


 - Enkel met IE laten werken of supporten we ook andere browsers?
	(python submit.py --url example.org --browser chrome ?????)
 - Enkel in Virtualbox laten werken of overal


\section{Approach and methods}

uitleggen van hoe we die concurrency gaan doen

gebruik maken van wat al bestaat: Cuckoo
open source op github


\section{Requirements}

As we will use and change Cuckoo, we require the Cuckoo source code as well as all dependencies needed by Cuckoo. Furthermore, a virtualized environment such as Virtualbox is needed with a Microsoft Windows virtual machine. Internet Explorer 8 will be considered as the minimum version of the browser that we use.

\section{Planning}

A meeting with the supervisors is planned every wednesday afternoon. Therefore in week 1, we set the deadline of the proposal on wednesday so it can be reviewed by our supervisors and be changed accordingly to the feedback given. At the end of week 1 and the beginning of week 2, time will be spend on devising an algorithm to implement concurrent analysis of URLs. After that, most of our time will be spent implementing our algorithm. In week 4, the report will be written and the presentation will be prepared.

\section{Expected results}

To implement our research in a proof of concept, we will be changing the Cuckoo source code to be able to track multiple URLs in one virtual machine. First, multiple URL support should be added to allow multiple URLs in the virtual environment. Secondly, we will have to implement the algorithm to track the changes that are the result of browsing to an URL. Finally, we will implement a man-in-the-middle to be able to view the contents of encrypted SSL/TLS tunnels.\\

We also want to drastically speed up Cuckoo to make it more useable for checking thousands of URLs. Our goal is to increase the performance tenfold.

\section{Ethical issues}

Our research contains no major ethical issues as it does not include working with personally identifiable information. Malware, if any, will be runned in a controlled virtual environment. Afterwards , the virtual machine will be destroyed.

\bibliographystyle{ieeetr}
\bibliography{proposal}

\end{document}
