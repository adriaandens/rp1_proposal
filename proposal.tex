\documentclass{scrartcl}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage[parfill]{parskip}

\title{Improving drive-by download detection: boot. visit$^n$. kill. repeat.}
\subtitle{An RP1 proposal,\\Master in System and Network Engineering}
\author{
  Adriaan Dens\\
   \texttt{adriaan.dens@os3.nl}
  \and
  Martijn Bogaard\\
   \texttt{martijn.bogaard@os3.nl}
}

\begin{document}
\maketitle

\section{Introduction}

In the digital world of today, malware is still a massive and growing problem. While back in the day it was used to annoy users and system administrators, nowadays it's used for extortion, cyber espionage and surveillance by criminal groups and rivaling governments. One of the main risk factors to get infected with malware is a drive-by download while visiting a normal website, for example because the website was hacked and infected. 

Several major websites in the Netherlands have been hacked in the past and started to serve malware as a consequence. While this was usually detected in a matter of hours, this still resulted in thousands of infections.

In many cases, it was not the actual website but one of the advertisement networks that was hacked or infiltrated and which subsequently started serving malicious code hidden in innocently looking advertisement code.

%In many of the cases where one of the bigger national websites was involved, it was not the actual website but one of the advertisement networks that are used by such websites that was hacked or infiltrated by hiding in innocent looking advertisement malicious code. Such advertisements can then be served on many different websites.

National CERT organisations have a growing interest in an early detection of such threats. While automated systems to scan websites already exist, one of the main difficulties is the time needed to analyse a single website and the maintenance needed to keep it up-to-date for the latest threats.

\section{Research Question}
At the request of the National Cyber Security Center (NCSC), this research project will focus on the question:\\ %how it would be possible to dramatically improve the efficiency of scanning websites while maintaining the ability to pinpoint a single website as the source of an infection.

\textit{How can we concurrently visit multiple URLs in one virtual machine and still be able to determine which URL was responsible for a malicious advertisement?}
%\textit{How can we do massive malware scanning in a timely fashion?}

\section{Related work}

Most existing systems for malware detection are passive or only able to scan a single website at a time. Several approaches for such systems are documented\cite{auto_malware} in the literature\cite{}. Cuckoo is one of the most popular dynamic analysis systems and used by Virustotal and Malwr. The malware runs in a virtual machine and its behaviour is analysed.

The predecessor of NCSC started in 2007 with the development of their own system (Honeyspider network) for the dynamic analysis of websites. This system crawled the top X websites of the Netherlands on a daily base. The downside of this system is that it requires a lot of maintenance and hence it started to become outdated.

In 2011, the development was started of the second generation of this platform. This time, it was decided to develop the platform as part of a joint venture between NASK/CERT Polska (Poland) and NCSC. This system is build around existing solutions like Cuckoo.

\section{Scope}

Inside the scope of this project is figuring out and implementing an algorithm to concurrently run multiple URLs in one virtual machine. Secondly, we will implement a man-in-the-middle to decrypt HTTPS traffic. Multiple browser support is not in the scope of this project, we will only implement our changes in the default Windows browser, Internet Explorer.

\section{Requirements}

As we will use and change Cuckoo, we require the Cuckoo source code as well as all dependencies needed by Cuckoo. Furthermore, a virtualized environment such as Virtualbox is needed with a Microsoft Windows virtual machine. Internet Explorer 8 will be considered as the minimum version of the browser that we use.

\section{Planning}

A meeting with the supervisors is planned every wednesday afternoon. Therefore in week 1, we set the deadline of the proposal on wednesday so it can be reviewed by our supervisors and be changed accordingly to the feedback given. At the end of week 1 and the beginning of week 2, time will be spend on devising an algorithm to implement concurrent analysis of URLs. After that, most of our time will be spent implementing our algorithm. In week 4, the report will be written and the presentation will be prepared.

\section{Expected results}

To implement our research in a proof of concept, we will be changing the Cuckoo source code to be able to track multiple URLs in one virtual machine. First, multiple URL support should be added to allow multiple URLs in the virtual environment. Secondly, we will have to implement the algorithm to track the changes that are the result of browsing to an URL. Finally, we will implement a man-in-the-middle to be able to view the contents of encrypted SSL/TLS tunnels.\\

We also want to drastically speed up Cuckoo to make it more useable for checking thousands of URLs. Our goal is to increase the performance tenfold.

\section{Ethical issues}

Our research contains no major ethical issues as it does not include working with personally identifiable information. Malware, if any, will be runned in a controlled virtual environment. Afterwards , the virtual machine will be destroyed.

\bibliographystyle{ieeetr}
\bibliography{proposal}

\end{document}
